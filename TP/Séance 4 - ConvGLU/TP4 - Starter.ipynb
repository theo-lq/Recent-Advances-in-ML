{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP4 - ConvGLU\n",
    "\n",
    "Dans cette séance on souhaite implémenter plusieurs des techniques que l'on a vu en cours. Et particulièrement, on se pose la question de l'adaptation de couches GLU au réseau convolutionnel : et si on remplaçait les couches denses par des couches convolutionnelles ? \n",
    "\n",
    "Pour tester cette idée, qui peut ne rien donner, nous allons à nouveau travailler avec le dataset [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) pour pouvoir comparer un peu aux TP précédents. Commençons par importer les données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf \n",
    "import tensorflow_datasets as tfds\n",
    "import keras\n",
    "import keras_cv\n",
    "\n",
    "dataset = tfds.load(\"cifar10\", as_supervised=True)\n",
    "dataset_train, dataset_test = dataset[\"train\"], dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rappellons que les deux datasets sont des [tensorflow dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset). Nous allons pré-traiter les données de la même manière."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (64, 64)\n",
    "n_class = 102\n",
    "batch_size = 526\n",
    "\n",
    "def process_dataset(image, label):\n",
    "    image = tf.image.resize(image, image_size)\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    label = tf.one_hot(label, n_class)\n",
    "    return {\"images\": image, \"labels\": label}\n",
    "\n",
    "dataset_train = dataset_train.map(process_dataset).batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avions également une fonction qui nous permet de visualiser le dataset :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_dataset(dataset, title, figsize=(6, 6)):\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    for index, sample in enumerate(iter(dataset.take(9))):\n",
    "        images = sample[\"images\"]\n",
    "        plt.subplot(3, 3, index + 1)\n",
    "        plt.imshow(images[0].numpy().astype(\"uint8\"))\n",
    "        plt.axis(\"off\")\n",
    "    \n",
    "    plt.suptitle(title, fontsize=14)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_dataset(dataset_train, \"Cifar-10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation du dataset\n",
    "\n",
    "Avant de travailler sur l'architecture, fixons l'augmentation du dataset. Notons que, pour le moment, dans la fonction `process_dataset` nous n'avons pas normaliser les données entre 0 et 1.\n",
    "\n",
    "**Consigne** : À l'aide du TP précédent, utiliser quelques couches d'[augmentations](https://keras.io/api/layers/preprocessing_layers/image_augmentation/) (et/ou aussi [ici](https://keras.io/api/keras_cv/layers/augmentation/)) pour construire un ensemble d'augmentation. Puis visualiser les changements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Couche ConvGLU\n",
    "\n",
    "On souhaite tester l'idée des Gated Linear Unit (GLU) avec des couches de convolutions. Cependant, il n'existe pas de fonctions déjà prête dans Keras pour le faire. Nous allons devoir créer notre propre couche.\n",
    "\n",
    "**Consigne** : Construire la classe `ConvGLU` qui hérite de [`keras.layers.Layer`](https://keras.io/api/layers/). Elle prendra en paramètre le chaîne de caractère *activation* qui correspond à la fonction d'activation que l'on souhaite utiliser pour le côté *Gated*. On utilisera également les *kwargs* pour passer des paramètres classiques de keras.\n",
    "Nous n'avons qu'a spécifier la méthode `call` qui correspond à l'appel de la couche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons l'ensemble des briques pour pouvoir construire notre réseau. \n",
    "\n",
    "**Consigne** : Définir une fonction `get_model` qui prend en paramètre une chaîne de caractère *activation* qui correspond à la fonction d'activation à transmettre à la couche `ConvGLU`. La fonction renverra le modèle à entraîner. Commencer le modèle par une couche [`keras.layers.Input`](https://keras.io/api/layers/core_layers/input/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Échéancier cosinus\n",
    "\n",
    "On souhaite entraîner le modèle avec un échéancier cosinus. L'implémentation keras est réalisée avec un changement du learning rate à chaque batch, et on souhaiterai avoir un paramétrage par époques.\n",
    "\n",
    "**Consigne** : Définir une fonction `get_cosine_scheduler` qui prend en paramètre :\n",
    "* *batch_per_epoch*: le nombre de batch par époques\n",
    "* *decay_epochs*: le nombre d'époque où le learning rate va baisser\n",
    "* *warmup_epochs*: le nombre d'époque de chauffe\n",
    "\n",
    "On intégrera également les *kwargs* pour que l'on puisse passer les paramètres clé à la classe [`CosineDecay`](https://keras.io/api/optimizers/learning_rate_schedules/cosine_decay/), par exemple le learning rate, la réduction du learning rate... La fonction renverra l'objet *schedule* pour pouvoir l'intégrer dans la description de l'entraînement du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entraînement\n",
    "\n",
    "Nous avons l'ensemble des briques nécessaire pour pouvoir tester notre idée. Il faut, avant de lancer l'entraînement, modifier le preprocessing pour normaliser les données.\n",
    "\n",
    "**Consigne** : Modifier la fonction `process_dataset` pour qu'elle normalise les données entre 0 et 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consigne** : Adapter en conséquence la liste des augmentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consigne** : Définir une fonction `get_dataset` qui renvoie le dataset d'entraînement et de validation, avec l'application des augmentations pour le dataset d'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consigne** : Entraîner le modèle, puis commenter ses performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
