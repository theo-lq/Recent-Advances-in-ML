%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for ThÃ©o LopÃ¨s-Quintas at 2024-05-09 00:49:53 +0200 


%% Saved with string encoding Unicode (UTF-8) 



@article{shumailov2023curse,
	author = {Shumailov, Ilia and Shumaylov, Zakhar and Zhao, Yiren and Gal, Yarin and Papernot, Nicolas and Anderson, Ross},
	date-added = {2024-05-09 00:34:15 +0200},
	date-modified = {2024-05-09 00:34:39 +0200},
	journal = {arXiv preprint arXiv:2305.17493},
	keywords = {LLM},
	title = {The curse of recursion: Training on generated data makes models forget},
	year = {2023}}

@article{phi1,
	author = {Gunasekar, Suriya and Zhang, Yi and Aneja, Jyoti and Mendes, Caio C{\'e}sar Teodoro and Del Giorno, Allie and Gopi, Sivakanth and Javaheripi, Mojan and Kauffmann, Piero and de Rosa, Gustavo and Saarikivi, Olli and others},
	date-added = {2024-05-08 23:54:54 +0200},
	date-modified = {2024-05-08 23:55:21 +0200},
	journal = {arXiv preprint arXiv:2306.11644},
	keywords = {LLM},
	title = {Textbooks are all you need},
	year = {2023}}

@article{liu2024datasets,
	author = {Liu, Yang and Cao, Jiahuan and Liu, Chongyu and Ding, Kai and Jin, Lianwen},
	date-added = {2024-05-08 23:51:15 +0200},
	date-modified = {2024-05-08 23:51:33 +0200},
	journal = {arXiv preprint arXiv:2402.18041},
	keywords = {Datasets},
	title = {Datasets for Large Language Models: A Comprehensive Survey},
	year = {2024}}

@article{cite-key,
	author = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
	date-added = {2024-05-02 23:25:11 +0200},
	date-modified = {2024-05-02 23:25:26 +0200},
	keywords = {Tokens}}

@article{together2023redpajama,
	author = {Together Computer},
	date-modified = {2024-05-09 00:39:43 +0200},
	keywords = {Datasets},
	month = October,
	title = {RedPajama: an Open Dataset for Training Large Language Models},
	url = {https://github.com/togethercomputer/RedPajama-Data},
	year = 2023}

@article{penedo2024fineweb,
	author = {Penedo, Guilherme and Kydl{\'\i}{\v c}ek, Hynek and von Werra, Leandro and Wolf, Thomas},
	date-modified = {2024-05-09 00:39:32 +0200},
	doi = {10.57967/hf/2092},
	keywords = {Datasets},
	month = April,
	title = {FineWeb},
	url = {https://huggingface.co/datasets/HuggingFaceFW/fineweb},
	year = 2024}

@article{TheStack,
	author = {Kocetkov, Denis and Li, Raymond and Ben Allal, Loubna and Li, Jia and Mou,Chenghao and Mu{\~n}oz Ferrandis, Carlos and Jernite, Yacine and Mitchell, Margaret and Hughes, Sean and Wolf, Thomas and Bahdanau, Dzmitry and von Werra, Leandro and de Vries, Harm},
	date-added = {2024-05-02 19:39:33 +0200},
	date-modified = {2024-05-02 19:40:03 +0200},
	journal = {Preprint},
	keywords = {Datasets},
	title = {The Stack: 3 TB of permissively licensed source code},
	year = {2022}}

@misc{ProofPile2,
	author = {Zhangir Azerbayev and Hailey Schoelkopf and Keiran Paster and Marco Dos Santos and Stephen McAleer and Albert Q. Jiang and Jia Deng and Stella Biderman and Sean Welleck},
	date-added = {2024-05-02 19:37:49 +0200},
	date-modified = {2024-05-02 19:38:34 +0200},
	keywords = {Datasets},
	title = {Llemma: An Open Language Model For Mathematics},
	year = {2023}}

@article{C4,
	author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
	date-added = {2024-05-02 19:32:01 +0200},
	date-modified = {2024-05-02 19:32:26 +0200},
	journal = {Journal of machine learning research},
	keywords = {Datasets},
	title = {Exploring the limits of transfer learning with a unified text-to-text transformer},
	year = {2020}}

@article{ThePile,
	author = {Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and others},
	date-added = {2024-05-02 19:29:38 +0200},
	date-modified = {2024-05-02 19:30:11 +0200},
	journal = {arXiv preprint arXiv:2101.00027},
	keywords = {Datasets},
	title = {The pile: An 800gb dataset of diverse text for language modeling},
	year = {2020}}

@article{Mixtral8x7B,
	author = {Jiang, Albert Q and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Hanna, Emma Bou and Bressand, Florian and others},
	date-added = {2024-05-01 20:10:06 +0200},
	date-modified = {2024-05-01 20:10:28 +0200},
	journal = {arXiv preprint arXiv:2401.04088},
	keywords = {LLM},
	title = {Mixtral of experts},
	year = {2024}}

@article{Mistral7B,
	author = {Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
	date-added = {2024-05-01 20:09:25 +0200},
	date-modified = {2024-05-01 20:09:53 +0200},
	journal = {arXiv preprint arXiv:2310.06825},
	keywords = {LLM},
	title = {Mistral 7B},
	year = {2023}}

@article{GPT2,
	author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
	date-added = {2024-04-28 14:51:36 +0200},
	date-modified = {2024-05-02 19:35:07 +0200},
	journal = {OpenAI blog},
	keywords = {LLM, Datasets},
	title = {Language models are unsupervised multitask learners},
	year = {2019}}

@article{kudo2018sentencepiece,
	author = {Kudo, Taku and Richardson, John},
	date-added = {2024-04-27 18:45:11 +0200},
	date-modified = {2024-04-27 18:45:33 +0200},
	journal = {arXiv preprint arXiv:1808.06226},
	keywords = {Tokens},
	title = {Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing},
	year = {2018}}

@article{kudo2018subword,
	author = {Kudo, Taku},
	date-added = {2024-04-27 18:37:53 +0200},
	date-modified = {2024-04-27 18:38:13 +0200},
	journal = {arXiv preprint arXiv:1804.10959},
	keywords = {Tokens},
	title = {Subword regularization: Improving neural network translation models with multiple subword candidates},
	year = {2018}}

@article{wu2016google,
	author = {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and others},
	date-added = {2024-04-27 18:26:49 +0200},
	date-modified = {2024-04-27 18:27:10 +0200},
	journal = {arXiv preprint arXiv:1609.08144},
	keywords = {Tokens},
	title = {Google's neural machine translation system: Bridging the gap between human and machine translation},
	year = {2016}}

@inproceedings{schuster2012japanese,
	author = {Schuster, Mike and Nakajima, Kaisuke},
	booktitle = {2012 IEEE international conference on acoustics, speech and signal processing (ICASSP)},
	date-added = {2024-04-27 18:24:06 +0200},
	date-modified = {2024-04-27 18:24:35 +0200},
	keywords = {Tokens},
	title = {Japanese and korean voice search},
	year = {2012}}

@article{gage1994new,
	author = {Gage, Philip},
	date-added = {2024-04-27 18:04:12 +0200},
	date-modified = {2024-04-27 18:04:36 +0200},
	journal = {The C Users Journal},
	keywords = {Tokens},
	title = {A new algorithm for data compression},
	year = {1994}}

@article{sennrich2015neural,
	author = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
	date-added = {2024-04-27 18:00:46 +0200},
	date-modified = {2024-04-27 18:01:12 +0200},
	journal = {arXiv preprint arXiv:1508.07909},
	keywords = {Tokens},
	title = {Neural machine translation of rare words with subword units},
	year = {2015}}

@article{devlin2018bert,
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	date-added = {2024-04-27 17:49:23 +0200},
	date-modified = {2024-04-27 17:49:53 +0200},
	journal = {arXiv preprint arXiv:1810.04805},
	keywords = {Tokens},
	title = {Bert: Pre-training of deep bidirectional transformers for language understanding},
	year = {2018}}

@article{aiindex2024,
	author = {Maslej , Nestor and Fattorini , Loredana and Perrault, Raymond and Parli, Vanessa and Reuel, Anka and Brynjolfsson, Erik and Etchemendy, John and Ligett, Katrina and Lyons, Terah and Manyika, James and Niebles, Juan Carlos and Shoham, Yoav and Wald, Russell and Clark, Jack},
	date-added = {2024-04-21 09:42:00 +0200},
	date-modified = {2024-04-21 09:48:43 +0200},
	journal = {AI Index Steering Committee, Institute for Human-Centered AI, Stanford University},
	title = {The AI Index 2024 Annual Report},
	year = {2024}}

@inproceedings{liu2022convnet,
	author = {Liu, Zhuang and Mao, Hanzi and Wu, Chao-Yuan and Feichtenhofer, Christoph and Darrell, Trevor and Xie, Saining},
	booktitle = {Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
	date-added = {2024-04-21 08:35:51 +0200},
	date-modified = {2024-04-21 08:36:28 +0200},
	keywords = {LLM Advance},
	pages = {11976--11986},
	title = {A convnet for the 2020s},
	year = {2022}}

@article{wu2020visual,
	author = {Wu, Bichen and Xu, Chenfeng and Dai, Xiaoliang and Wan, Alvin and Zhang, Peizhao and Yan, Zhicheng and Tomizuka, Masayoshi and Gonzalez, Joseph and Keutzer, Kurt and Vajda, Peter},
	date-added = {2024-04-14 12:24:06 +0200},
	date-modified = {2024-04-14 12:38:12 +0200},
	journal = {arXiv preprint arXiv:2006.03677},
	keywords = {LLM Advance},
	title = {Visual transformers: Token-based image representation and processing for computer vision},
	year = {2020}}

@inproceedings{bello2019attention,
	author = {Bello, Irwan and Zoph, Barret and Vaswani, Ashish and Shlens, Jonathon and Le, Quoc V},
	booktitle = {Proceedings of the IEEE/CVF international conference on computer vision},
	date-added = {2024-04-14 12:17:25 +0200},
	date-modified = {2024-04-14 12:38:04 +0200},
	keywords = {LLM Advance},
	pages = {3286--3295},
	title = {Attention augmented convolutional networks},
	year = {2019}}

@article{fashionMNIST,
	author = {Xiao , Han and Rasul, Kashif and Vollgraf, Roland},
	date-added = {2024-04-14 11:49:07 +0200},
	date-modified = {2024-04-14 11:49:19 +0200},
	journal = {CoRR},
	keywords = {Vision},
	title = {Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms},
	year = {2017}}

@article{lecun2010mnist,
	author = {LeCun, Yann and Cortes, Corinna and Burges, CJ},
	date-added = {2024-04-14 11:48:32 +0200},
	date-modified = {2024-04-14 11:48:54 +0200},
	journal = {ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},
	keywords = {Vision},
	title = {MNIST handwritten digit database},
	volume = {2},
	year = {2010}}

@article{lecun1998gradient,
	author = {LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
	date-added = {2024-04-14 11:47:43 +0200},
	date-modified = {2024-04-14 11:48:01 +0200},
	journal = {Proceedings of the IEEE},
	keywords = {Vision},
	title = {Gradient-based learning applied to document recognition},
	year = {1998}}

@inproceedings{liu2021swin,
	author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
	booktitle = {Proceedings of the IEEE/CVF international conference on computer vision},
	date-added = {2024-04-14 11:25:44 +0200},
	date-modified = {2024-04-14 11:26:14 +0200},
	keywords = {LLM Advance},
	title = {Swin transformer: Hierarchical vision transformer using shifted windows},
	year = {2021}}

@article{dosovitskiy2020image,
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
	date-added = {2024-04-14 11:20:31 +0200},
	date-modified = {2024-04-14 11:21:11 +0200},
	journal = {arXiv preprint arXiv:2010.11929},
	keywords = {LLM Advance},
	title = {An image is worth 16x16 words: Transformers for image recognition at scale},
	year = {2020}}

@article{Gemma,
	author = {Team, Gemma and Mesnard, Thomas and Hardin, Cassidy and Dadashi, Robert and Bhupatiraju, Surya and Pathak, Shreya and Sifre, Laurent and Rivi{\`e}re, Morgane and Kale, Mihir Sanjay and Love, Juliette and others},
	date-added = {2024-04-13 22:21:19 +0200},
	date-modified = {2024-04-13 22:23:14 +0200},
	journal = {arXiv preprint arXiv:2403.08295},
	keywords = {LLM},
	title = {Gemma: Open models based on gemini research and technology},
	year = {2024}}

@article{nakkiran2019more,
	author = {Nakkiran, Preetum},
	date-added = {2024-04-07 20:39:06 +0200},
	date-modified = {2024-04-07 20:39:33 +0200},
	journal = {arXiv preprint arXiv:1912.07242},
	keywords = {DoubleDescent},
	title = {More data can hurt for linear regression: Sample-wise double descent},
	year = {2019}}

@article{hastie2022surprises,
	author = {Hastie, Trevor and Montanari, Andrea and Rosset, Saharon and Tibshirani, Ryan J},
	date-added = {2024-04-06 14:29:43 +0200},
	date-modified = {2024-04-06 14:30:47 +0200},
	journal = {Annals of statistics},
	keywords = {DoubleDescent},
	number = {2},
	pages = {949},
	title = {Surprises in high-dimensional ridgeless least squares interpolation},
	volume = {50},
	year = {2022}}

@article{belkin2020two,
	author = {Belkin, Mikhail and Hsu, Daniel and Xu, Ji},
	date-added = {2024-04-06 14:27:48 +0200},
	date-modified = {2024-04-06 14:29:38 +0200},
	journal = {SIAM Journal on Mathematics of Data Science},
	keywords = {DoubleDescent},
	number = {4},
	pages = {1167--1180},
	title = {Two models of double descent for weak features},
	volume = {2},
	year = {2020}}

@article{simonyan2014very,
	author = {Simonyan, Karen and Zisserman, Andrew},
	date-added = {2024-01-29 10:20:49 +0100},
	date-modified = {2024-04-14 11:26:58 +0200},
	journal = {arXiv preprint arXiv:1409.1556},
	keywords = {Vision},
	title = {Very deep convolutional networks for large-scale image recognition},
	year = {2014}}

@article{krizhevsky2012imagenet,
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	date-added = {2024-01-29 10:20:44 +0100},
	date-modified = {2024-01-29 10:21:06 +0100},
	journal = {Advances in neural information processing systems},
	keywords = {Vision},
	title = {Imagenet classification with deep convolutional neural networks},
	year = {2012}}

@inproceedings{szegedy2015going,
	author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
	booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
	date-added = {2024-01-29 10:20:25 +0100},
	date-modified = {2024-01-29 10:20:59 +0100},
	keywords = {Vision},
	title = {Going deeper with convolutions},
	year = {2015}}

@inproceedings{balduzzi2017shattered,
	author = {Balduzzi, David and Frean, Marcus and Leary, Lennox and Lewis, JP and Ma, Kurt Wan-Duo and McWilliams, Brian},
	booktitle = {International Conference on Machine Learning},
	date-added = {2024-01-29 10:02:31 +0100},
	date-modified = {2024-01-29 10:03:19 +0100},
	keywords = {ResNet},
	organization = {PMLR},
	title = {The shattered gradients problem: If resnets are the answer, then what is the question?},
	year = {2017}}

@article{zhang2019fixup,
	author = {Zhang, Hongyi and Dauphin, Yann N and Ma, Tengyu},
	date-added = {2024-01-29 09:59:20 +0100},
	date-modified = {2024-01-29 10:03:38 +0100},
	journal = {ICLR},
	keywords = {ResNet},
	title = {Fixup initialization: Residual learning without normalization},
	year = {2019}}

@article{hinton2012improving,
	author = {Hinton, Geoffrey E and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R},
	date-added = {2024-01-06 01:04:29 +0100},
	date-modified = {2024-01-06 01:04:39 +0100},
	journal = {arXiv preprint arXiv:1207.0580},
	keywords = {G{\'e}n{\'e}ral},
	title = {Improving neural networks by preventing co-adaptation of feature detectors},
	year = {2012}}

@article{liu2019variance,
	abstract = {Introduit RAdam qui int{\`e}gre une partie de warmup. Il est montr{\'e} que le warmup peut {\^e}tre compris comme une mani{\`e}re de r{\'e}duire la variance de l'initialisation},
	author = {Liu, Liyuan and Jiang, Haoming and He, Pengcheng and Chen, Weizhu and Liu, Xiaodong and Gao, Jianfeng and Han, Jiawei},
	date-added = {2023-09-30 10:42:08 +0200},
	date-modified = {2023-09-30 10:42:48 +0200},
	journal = {arXiv preprint arXiv:1908.03265},
	keywords = {Optimizer},
	title = {On the variance of the adaptive learning rate and beyond},
	year = {2019}}

@inproceedings{choromanska2015loss,
	annote = {Etudie et justifie que la majorit{\'e} des minimaux locaux se Â« valent Â» dans un r{\'e}seau de neurone et donc qu'obtenir au moins un de ces minimaux suffit : pas besoin du grand minimum},
	author = {Choromanska, Anna and Henaff, Mikael and Mathieu, Michael and Arous, G{\'e}rard Ben and LeCun, Yann},
	booktitle = {Artificial intelligence and statistics},
	date-added = {2023-09-30 00:07:29 +0200},
	date-modified = {2023-09-30 00:10:40 +0200},
	keywords = {G{\'e}n{\'e}ral},
	organization = {PMLR},
	title = {The loss surfaces of multilayer networks},
	year = {2015}}

@inproceedings{maas2013rectifier,
	author = {Maas, Andrew L and Hannun, Awni Y and Ng, Andrew Y and others},
	booktitle = {Proc. icml},
	date-added = {2023-09-07 00:14:56 +0200},
	date-modified = {2023-09-07 00:15:18 +0200},
	keywords = {Activation},
	title = {Rectifier nonlinearities improve neural network acoustic models},
	year = {2013}}

@inproceedings{he2016deep,
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
	date-added = {2023-09-03 15:52:10 +0200},
	date-modified = {2023-09-03 15:52:30 +0200},
	keywords = {ResNet},
	title = {Deep residual learning for image recognition},
	year = {2016}}

@inproceedings{huang2016deep,
	author = {Huang, Gao and Sun, Yu and Liu, Zhuang and Sedra, Daniel and Weinberger, Kilian Q},
	booktitle = {Computer Vision--ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11--14, 2016, Proceedings, Part IV 14},
	date-added = {2023-09-02 14:36:08 +0200},
	date-modified = {2023-09-02 14:36:31 +0200},
	keywords = {GoodStart},
	title = {Deep networks with stochastic depth},
	year = {2016}}

@inproceedings{schmidt2021descending,
	abstract = {Benchmark 15 optimizers standard et montre qu'il n'y a pas de concensus global},
	annote = {Â« Perhaps the most important takeaway from our study is hidden in plain sight: the field is in danger of being drowned by noise. Different optimizers exhibit a surprisingly similar performance distribution compared to a single method that is re-tuned or simply re-run with different random seeds. It is thus questionable how much insight the development of new methods yields, at least if they are conceptually and functionally close to the existing population. Â»},
	author = {Schmidt, Robin M and Schneider, Frank and Hennig, Philipp},
	booktitle = {International Conference on Machine Learning},
	date-added = {2023-09-01 18:23:25 +0200},
	date-modified = {2023-09-02 02:24:27 +0200},
	keywords = {Optimizer},
	organization = {PMLR},
	title = {Descending through a crowded valley-benchmarking deep learning optimizers},
	year = {2021}}

@article{krogh1991simple,
	annote = {Montre qu'il vaut mieux avoir de petits poids pour un r{\'e}seau qui g{\'e}n{\'e}ralise mieux},
	author = {Krogh, Anders and Hertz, John},
	date-added = {2023-09-01 01:21:36 +0200},
	date-modified = {2023-09-02 02:14:48 +0200},
	journal = {Advances in neural information processing systems},
	keywords = {G{\'e}n{\'e}ral},
	title = {A simple weight decay can improve generalization},
	year = {1991}}

@article{geman1992neural,
	annote = {Â« Much of the excitement about articial neural networks revolves around the promise to avoid the tedious, difficult, and generally expensive process of articulating heuristics and rules for machines that are to perform nontrivial perceptual and cognitive tasks, such as for vision systems and expert systems. We would naturally prefer to `teach' our machines by example, and would hope that a good learning algorithm would `discover' the various heuritics and rules that apply to the task at hand. Â»
},
	author = {Geman, Stuart and Bienenstock, Elie and Doursat, Ren{\'e}},
	date-added = {2023-09-01 00:58:32 +0200},
	date-modified = {2023-09-01 00:59:11 +0200},
	journal = {Neural computation},
	keywords = {G{\'e}n{\'e}ral},
	title = {Neural networks and the bias/variance dilemma},
	year = {1992}}

@article{zhou2023lima,
	author = {Zhou, Chunting and Liu, Pengfei and Xu, Puxin and Iyer, Srini and Sun, Jiao and Mao, Yuning and Ma, Xuezhe and Efrat, Avia and Yu, Ping and Yu, Lili and others},
	date-added = {2023-08-30 01:58:55 +0200},
	date-modified = {2023-08-30 01:59:25 +0200},
	journal = {arXiv preprint arXiv:2305.11206},
	keywords = {LLM Advance},
	title = {Lima: Less is more for alignment},
	year = {2023}}

@article{pedregosa_sklearn,
	author = {Pedregosa, Fabian and Varoquaux, Ga{\"e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and others},
	date-added = {2023-08-30 00:29:24 +0200},
	date-modified = {2023-08-30 00:29:34 +0200},
	journal = {The journal of machine learning research},
	keywords = {G{\'e}n{\'e}ral},
	title = {Scikit-learn: Machine learning in Python},
	year = {2011}}

@article{hinton2012neural,
	abstract = {Introduction de l'optimizer RMSProp},
	author = {Hinton, Geoffrey and Srivastava, Nitish and Swersky, Kevin},
	date-added = {2023-08-29 00:55:54 +0200},
	date-modified = {2023-09-02 02:25:23 +0200},
	journal = {Cited on},
	keywords = {Optimizer},
	title = {Neural networks for machine learning lecture 6a overview of mini-batch gradient descent},
	year = {2012}}

@article{duchi2011adaptive,
	abstract = {Pr{\'e}sentation de AdaGrad},
	author = {Duchi, John and Hazan, Elad and Singer, Yoram},
	date-added = {2023-08-28 19:48:26 +0200},
	date-modified = {2023-09-02 02:25:30 +0200},
	journal = {Journal of machine learning research},
	keywords = {Optimizer},
	title = {Adaptive subgradient methods for online learning and stochastic optimization},
	year = {2011}}

@article{olson1980profile,
	abstract = {Exp{\'e}rience sur le vivant pour prouver le bon d{\'e}but d'un r{\'e}seau de neurone avec achille2017Critical},
	author = {Olson, Carl R and Freeman, RD},
	date-added = {2023-08-26 12:38:08 +0200},
	date-modified = {2023-09-02 02:19:12 +0200},
	journal = {Experimental Brain Research},
	keywords = {GoodStart},
	title = {Profile of the sensitive period for monocular deprivation in kittens},
	year = {1980}}

@article{mitchell1988extent,
	abstract = {Exp{\'e}rience sur le vivant pour prouver le bon d{\'e}but d'un r{\'e}seau de neurone avec achille2017Critical},
	author = {Mitchell, Donald E},
	date-added = {2023-08-26 12:34:37 +0200},
	date-modified = {2023-09-02 02:18:41 +0200},
	journal = {The Journal of physiology},
	keywords = {GoodStart},
	title = {The extent of visual recovery from early monocular or binocular visual deprivation in kittens},
	year = {1988}}

@article{giffin1978rate,
	abstract = {Exp{\'e}rience sur le vivant pour prouver le bon d{\'e}but d'un r{\'e}seau de neurone avec achille2017Critical},
	author = {Giffin, Fred and Mitchell, Donald E},
	date-added = {2023-08-26 12:33:48 +0200},
	date-modified = {2023-09-02 02:19:17 +0200},
	journal = {The Journal of Physiology},
	keywords = {GoodStart},
	title = {The rate of recovery of vision after early monocular deprivation in kittens},
	year = {1978}}

@article{popel2018training,
	author = {Popel, Martin and Bojar, Ond{\v{r}}ej},
	date-added = {2023-08-20 21:10:55 +0200},
	date-modified = {2023-08-20 21:11:54 +0200},
	journal = {arXiv preprint arXiv:1804.00247},
	keywords = {LLM Advance},
	title = {Training tips for the transformer model},
	year = {2018}}

@article{lecun1998efficient,
	author = {LeCun, Yann A and Bottou, L{\'e}on and Orr, Genevieve B and M{\"u}ller, Klaus-Robert},
	date-added = {2023-08-20 14:40:11 +0200},
	date-modified = {2023-08-20 14:42:05 +0200},
	journal = {Tricks of the Trade},
	keywords = {R{\'e}gularisation},
	title = {Efficient BackProp},
	year = {1998}}

@article{dawid1982well,
	author = {Dawid, A Philip},
	date-added = {2023-08-18 17:03:41 +0200},
	date-modified = {2023-08-18 17:04:12 +0200},
	journal = {Journal of the American Statistical Association},
	keywords = {Calibration},
	title = {The well-calibrated Bayesian},
	year = {1982}}

@article{gneiting2007strictly,
	author = {Gneiting, Tilmann and Raftery, Adrian E},
	date-added = {2023-08-15 22:58:34 +0200},
	date-modified = {2023-08-15 22:59:11 +0200},
	journal = {Journal of the American statistical Association},
	keywords = {Calibration},
	number = {477},
	pages = {359--378},
	title = {Strictly proper scoring rules, prediction, and estimation},
	volume = {102},
	year = {2007}}

@article{savage1971elicitation,
	abstract = {Lien dual entre une fonction de perte bien d{\'e}finie et un ensemble de fonction convexes},
	author = {Savage, Leonard J},
	date-added = {2023-08-15 22:57:00 +0200},
	date-modified = {2023-09-02 02:05:42 +0200},
	journal = {Journal of the American Statistical Association},
	keywords = {Calibration},
	number = {336},
	pages = {783--801},
	title = {Elicitation of personal probabilities and expectations},
	volume = {66},
	year = {1971}}

@inproceedings{kakade2004deterministic,
	abstract = {Premi{\`e}re introduction de la smooth calibration error},
	annote = {Introduction de la notion de smooth calibration, mais pas sous la formulation moderne},
	author = {Kakade, Sham M and Foster, Dean P},
	booktitle = {International Conference on Computational Learning Theory},
	date-added = {2023-08-13 00:41:18 +0200},
	date-modified = {2023-09-02 02:04:43 +0200},
	keywords = {Calibration},
	organization = {Springer},
	pages = {33--48},
	title = {Deterministic calibration and Nash equilibrium},
	year = {2004}}

@inproceedings{gopalan2022low,
	abstract = {Contient les notations classiques de calibration, parle aussi de la smooth calibration error},
	author = {Gopalan, Parikshit and Kim, Michael P and Singhal, Mihir A and Zhao, Shengjia},
	booktitle = {Conference on Learning Theory},
	date-added = {2023-08-13 00:27:45 +0200},
	date-modified = {2023-09-02 02:05:28 +0200},
	keywords = {Calibration},
	organization = {PMLR},
	pages = {3193--3234},
	title = {Low-degree multicalibration},
	year = {2022}}

@article{xu2019understanding,
	author = {Xu, Jingjing and Sun, Xu and Zhang, Zhiyuan and Zhao, Guangxiang and Lin, Junyang},
	date-added = {2023-08-11 01:27:41 +0200},
	date-modified = {2023-08-11 01:28:11 +0200},
	journal = {Advances in Neural Information Processing Systems},
	keywords = {R{\'e}gularisation},
	title = {Understanding and improving layer normalization},
	year = {2019}}

@inproceedings{xiong2020layer,
	abstract = {Placement de la couche de normalisation dans l'architecture Transformers. Montre qu'avant le FFN cela permet de r{\'e}duire voire supprimer le warmup},
	author = {Xiong, Ruibin and Yang, Yunchang and He, Di and Zheng, Kai and Zheng, Shuxin and Xing, Chen and Zhang, Huishuai and Lan, Yanyan and Wang, Liwei and Liu, Tieyan},
	booktitle = {International Conference on Machine Learning},
	date-added = {2023-08-11 01:26:49 +0200},
	date-modified = {2023-09-02 02:25:58 +0200},
	keywords = {R{\'e}gularisation},
	organization = {PMLR},
	title = {On layer normalization in the transformer architecture},
	year = {2020}}

@article{zhang2019root,
	abstract = {Pr{\'e}sente RMSNorm},
	author = {Zhang, Biao and Sennrich, Rico},
	date-added = {2023-08-11 01:24:42 +0200},
	date-modified = {2023-09-02 02:26:42 +0200},
	journal = {Advances in Neural Information Processing Systems},
	keywords = {R{\'e}gularisation},
	title = {Root mean square layer normalization},
	volume = {32},
	year = {2019}}

@article{kaplan2020scaling,
	author = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
	date-added = {2023-08-03 12:06:47 +0200},
	date-modified = {2023-08-03 12:07:08 +0200},
	journal = {arXiv preprint arXiv:2001.08361},
	keywords = {LLM Advance},
	title = {Scaling laws for neural language models},
	year = {2020}}

@article{GOPHER,
	abstract = {Entreprise : Deepmind
Schedule : Cosine LR
Gradient clipping : Oui
Activation : GELU
Optimizer : Adam
Normalization : RMSNorm},
	author = {Rae, Jack W and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and others},
	date-added = {2023-08-03 11:25:04 +0200},
	date-modified = {2023-09-02 02:10:57 +0200},
	journal = {arXiv preprint arXiv:2112.11446},
	keywords = {LLM},
	title = {Scaling language models: Methods, analysis \& insights from training gopher},
	year = {2021}}

@article{vaswani2017attention,
	abstract = {Introduit l'architecture Transformers},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
	date-added = {2023-07-29 14:51:56 +0200},
	date-modified = {2023-09-02 02:21:53 +0200},
	journal = {Advances in neural information processing systems},
	keywords = {LLM Advance},
	title = {Attention is all you need},
	volume = {30},
	year = {2017}}

@inproceedings{pascanu2013difficulty,
	author = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
	booktitle = {International conference on machine learning},
	date-added = {2023-07-29 13:30:09 +0200},
	date-modified = {2023-07-29 13:30:57 +0200},
	keywords = {R{\'e}gularisation},
	organization = {PMLR},
	pages = {1310--1318},
	title = {On the difficulty of training recurrent neural networks},
	year = {2013}}

@article{GATO,
	abstract = {Entreprise : Deepmind
Schedule : Cosine LR
Gradient clipping : ?
Activation : GEGELU
Optimizer : AdamW
Normalization : PreNorm},
	annote = {Int{\'e}ressant : utilise stochastic depth},
	author = {Reed, Scott and Zolna, Konrad and Parisotto, Emilio and Colmenarejo, Sergio Gomez and Novikov, Alexander and Barth-Maron, Gabriel and Gimenez, Mai and Sulsky, Yury and Kay, Jackie and Springenberg, Jost Tobias and others},
	date-added = {2023-07-26 22:18:46 +0200},
	date-modified = {2023-09-02 02:10:48 +0200},
	journal = {arXiv preprint arXiv:2205.06175},
	keywords = {LLM},
	title = {A generalist agent},
	year = {2022}}

@article{GPT3,
	abstract = {Entreprise : Open AI
Schedule : Cosine LR
Gradient clipping : Oui
Activation : GELU
Optimizer : Adam
Normalization : Layer Norm},
	author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
	date-added = {2023-07-26 22:03:32 +0200},
	date-modified = {2023-09-02 02:11:06 +0200},
	journal = {Advances in Neural Information Processing Systems},
	keywords = {LLM},
	title = {Language models are few-shot learners},
	year = {2020}}

@article{Falcon,
	author = {Penedo, Guilherme and Malartic, Quentin and Hesslow, Daniel and Cojocaru, Ruxandra and Cappelli, Alessandro and Alobeidli, Hamza and Pannier, Baptiste and Almazrouei, Ebtesam and Launay, Julien},
	date-added = {2023-07-26 21:46:47 +0200},
	date-modified = {2024-05-02 19:35:40 +0200},
	journal = {arXiv preprint arXiv:2306.01116},
	keywords = {LLM,Datasets},
	title = {The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only},
	year = {2023}}

@article{LaMDa,
	abstract = {Entreprise : Google
Schedule : Noam LR
Gradient clipping : ?
Activation : GEGELU
Optimizer : Adam
Normalization : Layer Norm},
	annote = {Google},
	author = {Thoppilan, Romal and De Freitas, Daniel and Hall, Jamie and Shazeer, Noam and Kulshreshtha, Apoorv and Cheng, Heng-Tze and Jin, Alicia and Bos, Taylor and Baker, Leslie and Du, Yu and others},
	date-added = {2023-07-26 21:32:35 +0200},
	date-modified = {2023-09-02 02:11:12 +0200},
	journal = {arXiv preprint arXiv:2201.08239},
	keywords = {LLM},
	title = {Lamda: Language models for dialog applications},
	year = {2022}}

@article{LLaMa2,
	abstract = {Entreprise : Meta
Schedule : Cosine
Gradient clipping : Oui
Activation : SwiGLU
Optimizer : AdamW
Normalization : RMSNorm},
	annote = {Meta},
	author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
	date-added = {2023-07-26 21:31:11 +0200},
	date-modified = {2023-09-02 02:13:23 +0200},
	journal = {arXiv preprint arXiv:2307.09288},
	keywords = {LLM},
	title = {Llama 2: Open Foundation and Fine-Tuned Chat Models},
	year = {2023}}

@article{LLaMa,
	abstract = {Entreprise : Meta
Schedule : Cosine
Gradient clipping : Oui
Activation : SwiGLU
Optimizer : AdamW
Normalization : RMSNorm},
	annote = {Meta},
	author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
	date-added = {2023-07-26 21:30:32 +0200},
	date-modified = {2023-09-02 02:13:52 +0200},
	journal = {arXiv preprint arXiv:2302.13971},
	keywords = {LLM},
	title = {Llama: Open and efficient foundation language models},
	year = {2023}}

@article{PaLM2,
	annote = {Entreprise : Google
Schedule : Cosine
Gradient clipping : ?
Activation : ?
Optimizer : ?
Normalization : ?},
	author = {Anil, Rohan and Dai, Andrew M and Firat, Orhan and Johnson, Melvin and Lepikhin, Dmitry and Passos, Alexandre and Shakeri, Siamak and Taropa, Emanuel and Bailey, Paige and Chen, Zhifeng and others},
	date-added = {2023-07-26 21:29:12 +0200},
	date-modified = {2023-09-02 02:14:04 +0200},
	journal = {arXiv preprint arXiv:2305.10403},
	keywords = {LLM},
	title = {Palm 2 technical report},
	year = {2023}}

@article{PaLM,
	abstract = {Entreprise : Google
Schedule : ?
Gradient clipping : ?
Activation : SwiGLU
Optimizer : AdaFactor
Normalization : ?},
	annote = {Google},
	author = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
	date-added = {2023-07-26 21:28:31 +0200},
	date-modified = {2023-09-02 02:12:00 +0200},
	journal = {arXiv preprint arXiv:2204.02311},
	keywords = {LLM},
	title = {Palm: Scaling language modeling with pathways},
	year = {2022}}

@article{Chinchilla,
	abstract = {Entreprise : Deepmind
Schedule : Cosine
Gradient clipping : Oui
Activation : GELU
Optimizer : AdamW
Normalization : RMSNorm},
	annote = {Etude tr{\`e}s int{\'e}ressante {\`a} faire sur la longueur optimale du cycle du cosine learning rate schedule
Deepmind},
	author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
	date-added = {2023-07-26 21:27:06 +0200},
	date-modified = {2023-09-02 02:12:44 +0200},
	journal = {arXiv preprint arXiv:2203.15556},
	keywords = {LLM},
	title = {Training compute-optimal large language models},
	year = {2022}}

@article{barron2021squareplus,
	abstract = {Propose une famille de fonction plus simple que softplus {\`a} calculer pour un ordinateur comme fonction d'activation},
	author = {Barron, Jonathan T},
	date-added = {2023-07-16 18:11:02 +0200},
	date-modified = {2023-09-06 23:57:07 +0200},
	journal = {arXiv preprint arXiv:2112.11687},
	keywords = {Activation},
	title = {Squareplus: A softplus-like algebraic rectifier},
	year = {2021}}

@article{barron2017continuously,
	abstract = {Propose la famille CELU comme fonctions d'activations},
	author = {Barron, Jonathan T},
	date-added = {2023-07-16 18:09:41 +0200},
	date-modified = {2023-09-06 23:57:39 +0200},
	journal = {arXiv preprint arXiv:1704.07483},
	keywords = {Activation},
	title = {Continuously differentiable exponential linear units},
	year = {2017}}

@article{misra2019mish,
	abstract = {Propose une nouvelle fonction d'activation},
	author = {Misra, Diganta},
	date-added = {2023-07-16 18:08:18 +0200},
	date-modified = {2023-09-06 23:56:51 +0200},
	journal = {arXiv preprint arXiv:1908.08681},
	keywords = {Activation},
	title = {Mish: A self regularized non-monotonic activation function},
	year = {2019}}

@inproceedings{dauphin2016language,
	abstract = {Premi{\`e}re utilisation des Gated Linear Unit pour la vision},
	author = {Dauphin, Yann N and Fan, Angela and Auli, Michael and Grangier, David},
	booktitle = {International conference on machine learning},
	date-added = {2023-07-15 17:57:26 +0200},
	date-modified = {2023-09-02 02:22:29 +0200},
	keywords = {LLM Advance},
	organization = {PMLR},
	pages = {933--941},
	title = {Language modeling with gated convolutional networks},
	year = {2016}}

@article{hendrycks2016gaussian,
	abstract = {Propose la fonction d'activation GELU},
	author = {Hendrycks, Dan and Gimpel, Kevin},
	date-added = {2023-07-15 17:55:57 +0200},
	date-modified = {2023-09-02 02:22:19 +0200},
	journal = {arXiv preprint arXiv:1606.08415},
	keywords = {LLM Advance},
	title = {Gaussian error linear units (gelus)},
	year = {2016}}

@article{ba2016layer,
	abstract = {Propose la LayerNorm en opposition {\`a} BatchNorm},
	author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
	date-added = {2023-07-14 15:10:14 +0200},
	date-modified = {2023-09-02 02:27:23 +0200},
	journal = {arXiv preprint arXiv:1607.06450},
	keywords = {R{\'e}gularisation},
	title = {Layer normalization},
	year = {2016}}

@article{blasiok2023loss,
	author = {B{\l}asiok, Jaros{\l}aw and Gopalan, Parikshit and Hu, Lunjia and Kalai, Adam Tauman and Nakkiran, Preetum},
	date-added = {2023-06-29 01:19:11 +0200},
	date-modified = {2023-06-29 01:19:36 +0200},
	journal = {arXiv preprint arXiv:2304.09424},
	keywords = {Calibration},
	title = {Loss minimization yields multicalibration for large neural networks},
	year = {2023}}

@inproceedings{blasiok2023unifying,
	abstract = {Propose un cadre rigoureux pour d{\'e}finir la calibration ainsi que les mesures de la distance {\`a} la calibration},
	author = {B{\l}asiok, Jaros{\l}aw and Gopalan, Parikshit and Hu, Lunjia and Nakkiran, Preetum},
	booktitle = {Proceedings of the 55th Annual ACM Symposium on Theory of Computing},
	date-added = {2023-06-29 01:17:59 +0200},
	date-modified = {2023-09-02 02:04:08 +0200},
	keywords = {Calibration},
	pages = {1727--1740},
	title = {A unifying theory of distance from calibration},
	year = {2023}}

@article{carrell2022calibration,
	author = {Carrell, Annabelle and Mallinar, Neil and Lucas, James and Nakkiran, Preetum},
	date-added = {2023-06-29 01:17:06 +0200},
	date-modified = {2023-06-29 01:17:27 +0200},
	journal = {arXiv preprint arXiv:2210.01964},
	keywords = {Calibration},
	title = {The Calibration Generalization Gap},
	year = {2022}}

@article{mallinar2022benign,
	author = {Mallinar, Neil and Simon, James and Abedsoltan, Amirhesam and Pandit, Parthe and Belkin, Misha and Nakkiran, Preetum},
	date-added = {2023-06-29 01:15:53 +0200},
	date-modified = {2023-06-29 01:16:33 +0200},
	journal = {Advances in Neural Information Processing Systems},
	keywords = {DoubleDescent},
	pages = {1182--1195},
	title = {Benign, tempered, or catastrophic: Toward a refined taxonomy of overfitting},
	volume = {35},
	year = {2022}}

@article{blasiok2023does,
	abstract = {La minimisation d'une fonction de perte induit la calibration si le gain {\`a} modifier le vecteur de pr{\'e}diction par une fonction 1-Lipschitzienne est faible},
	author = {B{\l}asiok, Jaros{\l}aw and Gopalan, Parikshit and Hu, Lunjia and Nakkiran, Preetum},
	date-added = {2023-06-29 01:11:42 +0200},
	date-modified = {2023-09-02 02:03:16 +0200},
	journal = {arXiv preprint arXiv:2305.18764},
	keywords = {Calibration},
	title = {When Does Optimizing a Proper Loss Yield Calibration?},
	year = {2023}}

@article{fan2018hierarchical,
	author = {Fan, Angela and Lewis, Mike and Dauphin, Yann},
	date-added = {2023-05-31 22:09:20 +0200},
	date-modified = {2024-04-14 10:55:56 +0200},
	journal = {arXiv preprint arXiv:1805.04833},
	keywords = {Tokens},
	title = {Hierarchical neural story generation},
	year = {2018}}

@article{shazeer2020glu,
	abstract = {Propose les Gated Linear Unit ainsi que plusieurs variantes dont GEGLU et SwiGLU},
	author = {Shazeer, Noam},
	date-added = {2023-05-31 22:03:18 +0200},
	date-modified = {2023-09-02 02:19:34 +0200},
	journal = {arXiv preprint arXiv:2002.05202},
	keywords = {LLM Advance},
	title = {Glu variants improve transformer},
	year = {2020}}

@article{ramachandran2017searching,
	abstract = {Recherche syst{\'e}matique de fonction d'activation. Retrouve la fonction SiLU nomm{\'e}e ici Swish},
	author = {Ramachandran, Prajit and Zoph, Barret and Le, Quoc V},
	date-added = {2023-05-31 22:01:28 +0200},
	date-modified = {2023-09-02 02:20:12 +0200},
	journal = {arXiv preprint arXiv:1710.05941},
	keywords = {LLM Advance},
	title = {Searching for activation functions},
	year = {2017}}

@article{holtzman2019curious,
	abstract = {Pr{\'e}sente les probl{\`e}mes de la g{\'e}n{\'e}ration de texte et aborde le temp{\'e}rature et nucleus sampling},
	author = {Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
	date-added = {2023-05-31 21:59:47 +0200},
	date-modified = {2024-04-14 10:56:23 +0200},
	journal = {arXiv preprint arXiv:1904.09751},
	keywords = {Tokens},
	title = {The curious case of neural text degeneration},
	year = {2019}}

@article{chen2023symbolic,
	abstract = {Recherche de mani{\`e}re syst{\'e}matique des algorithmes d'optimisation de r{\'e}seau de neurones. Abouti sur la pr{\'e}sentation de Lion},
	author = {Chen, Xiangning and Liang, Chen and Huang, Da and Real, Esteban and Wang, Kaiyuan and Liu, Yao and Pham, Hieu and Dong, Xuanyi and Luong, Thang and Hsieh, Cho-Jui and others},
	date-added = {2023-05-31 21:56:21 +0200},
	date-modified = {2023-09-02 02:23:33 +0200},
	journal = {arXiv preprint arXiv:2302.06675},
	keywords = {Optimizer},
	title = {Symbolic discovery of optimization algorithms},
	year = {2023}}

@article{liu2023dropout,
	annote = {Montre que le dropout permet de mieux lancer le r{\'e}seau de neurone pour g{\'e}n{\'e}raliser ensuite. Montre {\'e}galement que la stochastic depth est utile pour r{\'e}gulariser, mais pas au d{\'e}but},
	author = {Liu, Zhuang and Xu, Zhiqiu and Jin, Joseph and Shen, Zhiqiang and Darrell, Trevor},
	date-added = {2023-05-31 21:40:19 +0200},
	date-modified = {2023-09-02 02:15:34 +0200},
	journal = {arXiv preprint arXiv:2303.01500},
	keywords = {GoodStart},
	title = {Dropout Reduces Underfitting},
	year = {2023}}

@article{ioffe_BN,
	abstract = {Pr{\'e}sente BatchNorm},
	author = {Ioffe, Sergey and Szegedy, Christian},
	date-added = {2023-05-27 00:12:23 +0200},
	date-modified = {2023-09-02 02:27:37 +0200},
	journal = {International conference on machine learning},
	keywords = {R{\'e}gularisation},
	title = {Batch normalization: Accelerating deep network training by reducing internal covariate shift},
	year = {2015}}

@article{liu2018learning,
	author = {Liu, Weiyang and Lin, Rongmei and Liu, Zhen and Liu, Lixin and Yu, Zhiding and Dai, Bo and Song, Le},
	date-added = {2023-05-27 00:09:35 +0200},
	date-modified = {2023-05-27 00:10:09 +0200},
	journal = {Advances in neural information processing systems},
	keywords = {R{\'e}gularisation},
	title = {Learning towards minimum hyperspherical energy},
	volume = {31},
	year = {2018}}

@inproceedings{tan2022hyperspherical,
	author = {Tan, Cheng and Gao, Zhangyang and Wu, Lirong and Li, Siyuan and Li, Stan Z},
	booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	date-added = {2023-05-27 00:08:19 +0200},
	date-modified = {2023-05-27 00:09:07 +0200},
	keywords = {R{\'e}gularisation},
	pages = {7244--7255},
	title = {Hyperspherical consistency regularization},
	year = {2022}}

@article{loshchilov2017decoupled,
	abstract = {Introduit le weight decay},
	author = {Loshchilov, Ilya and Hutter, Frank},
	date-added = {2023-05-27 00:06:27 +0200},
	date-modified = {2023-10-01 18:54:43 +0200},
	journal = {International Conference on Learning Representations (ICLR)},
	keywords = {R{\'e}gularisation},
	title = {Decoupled weight decay regularization},
	year = {2019}}

@article{lemaitre_imblearn,
	author = {Lema{\^\i}tre, Guillaume and Nogueira, Fernando and Aridas, Christos K},
	date-added = {2023-05-26 23:44:00 +0200},
	date-modified = {2023-05-26 23:46:06 +0200},
	journal = {The journal of machine learning research},
	keywords = {Imbalanced},
	title = {Imbalanced-learn: A python toolbox to tackle the curse of imbalanced datasets in machine learning},
	year = {2017}}

@article{srivastava_dropout,
	abstract = {Pr{\'e}sente le dropout pour r{\'e}gulariser un r{\'e}seau de neurones},
	author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	date-added = {2023-05-26 23:43:33 +0200},
	date-modified = {2023-09-02 02:17:09 +0200},
	journal = {The journal of machine learning research},
	keywords = {GoodStart},
	title = {Dropout: a simple way to prevent neural networks from overfitting},
	year = {2014}}

@inproceedings{kingma_adam,
	abstract = {Pr{\'e}sente l'optimiser Adam},
	author = {Kingma, Diederik P and Ba, Jimmy},
	booktitle = {International Conference on Learning Representations (ICLR)},
	date-added = {2023-05-26 23:43:27 +0200},
	date-modified = {2023-10-01 18:54:34 +0200},
	journal = {arXiv preprint arXiv:1412.6980},
	keywords = {Optimizer},
	title = {Adam: A method for stochastic optimization},
	year = {2015}}

@article{nesterov_accelerated,
	abstract = {Description de l'acc{\'e}l{\'e}ration de Nesterov},
	author = {Nesterov, Youri},
	date-added = {2023-05-26 23:42:25 +0200},
	date-modified = {2023-09-02 02:25:38 +0200},
	journal = {Soviet Mathematics Doklady},
	keywords = {Optimizer},
	title = {A method of solving a convex programming problem with convergence rate $O(1/k^2)$},
	year = {1983}}

@article{bohr_atom,
	author = {Bohr, Niels},
	date-added = {2023-05-26 23:42:15 +0200},
	date-modified = {2023-05-26 23:42:15 +0200},
	journal = {The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science},
	keywords = {G{\'e}n{\'e}ral},
	title = {On the constitution of atoms and molecules},
	year = {1913}}

@article{turing_computing,
	author = {Turing, Alan M},
	date-added = {2023-05-26 23:42:15 +0200},
	date-modified = {2023-05-26 23:42:15 +0200},
	journal = {Mind},
	keywords = {G{\'e}n{\'e}ral},
	title = {Computing machinery and intelligence},
	year = {1950}}

@article{minsky_perceptron,
	author = {Minsky, Marvin and Papert, Seymour},
	date-added = {2023-05-26 23:42:15 +0200},
	date-modified = {2023-05-26 23:48:54 +0200},
	journal = {MIT press},
	keywords = {G{\'e}n{\'e}ral},
	title = {Perceptrons},
	year = {1969}}

@article{su_nesterov_differential,
	abstract = {Mod{\'e}lise l'acc{\'e}l{\'e}ration de Nesterov avec une {\'e}quation diff{\'e}rentielle et permette de l'{\'e}tudier un peu plus en profondeur.},
	author = {Su, Weijie and Boyd, Stephen and Candes, Emmanuel},
	date-added = {2023-05-26 23:41:42 +0200},
	date-modified = {2023-09-02 02:24:45 +0200},
	journal = {Advances in neural information processing systems},
	keywords = {Optimizer},
	title = {A differential equation for modeling Nesterov's accelerated gradient method: theory and insights},
	year = {2014}}

@article{more_sampling,
	author = {More, Ajinkya},
	date-added = {2023-05-26 23:40:59 +0200},
	date-modified = {2023-05-26 23:46:26 +0200},
	journal = {arXiv preprint arXiv:1608.06048},
	keywords = {Imbalanced},
	title = {Survey of resampling techniques for improving classification performance in unbalanced datasets},
	year = {2016}}

@article{klambauer_selu,
	abstract = {Pr{\'e}sente la fonction d'activation SeLU},
	author = {Klambauer, G{\"u}nter and Unterthiner, Thomas and Mayr, Andreas and Hochreiter, Sepp},
	date-added = {2023-05-26 23:39:56 +0200},
	date-modified = {2023-09-06 23:58:01 +0200},
	journal = {Advances in neural information processing systems},
	keywords = {Activation},
	title = {Self-normalizing neural networks},
	year = {2017}}

@article{grinsztajn_treeoutperform,
	author = {Grinsztajn, L{\'e}o and Oyallon, Edouard and Varoquaux, Ga{\"e}l},
	date-added = {2023-05-26 23:39:13 +0200},
	date-modified = {2023-05-26 23:45:56 +0200},
	journal = {arXiv preprint arXiv:2207.08815},
	keywords = {G{\'e}n{\'e}ral},
	title = {Why do tree-based models still outperform deep learning on tabular data?},
	year = {2022}}

@article{kaddour2022flat,
	author = {Kaddour, Jean and Liu, Linqing and Silva, Ricardo and Kusner, Matt J},
	date-added = {2023-05-21 14:39:20 +0200},
	date-modified = {2023-05-26 23:54:18 +0200},
	journal = {Advances in Neural Information Processing Systems},
	keywords = {Optimizer},
	title = {When Do Flat Minima Optimizers Work?},
	volume = {35},
	year = {2022}}

@inproceedings{bender2021dangers,
	author = {Bender, Emily M and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
	booktitle = {Proceedings of the 2021 ACM conference on fairness, accountability, and transparency},
	date-added = {2023-05-21 14:18:53 +0200},
	date-modified = {2024-04-14 10:56:10 +0200},
	keywords = {Tokens},
	pages = {610--623},
	title = {On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?ðŸ¦œ},
	year = {2021}}

@article{li2018Smooth,
	author = {Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
	date-added = {2023-05-21 14:08:38 +0200},
	date-modified = {2023-05-21 14:09:18 +0200},
	journal = {Advances in neural information processing systems},
	keywords = {ResNet},
	title = {Visualizing the loss landscape of neural nets},
	volume = {31},
	year = {2018}}

@article{balestriero_highdimension,
	author = {Balestriero, Randall and Pesenti, Jerome and LeCun, Yann},
	date-added = {2023-05-21 14:02:44 +0200},
	date-modified = {2023-05-21 14:04:09 +0200},
	journal = {arXiv preprint arXiv:2110.09485},
	keywords = {CurseOfDimensionnality},
	title = {Learning in high dimension always amounts to extrapolation},
	year = {2021}}

@article{belkin_DoubleDescent,
	author = {Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
	date-added = {2023-05-21 14:01:56 +0200},
	date-modified = {2023-05-21 14:03:34 +0200},
	journal = {Proceedings of the National Academy of Sciences},
	keywords = {DoubleDescent},
	title = {Reconciling modern machine-learning practice and the classical bias-variance trade-off},
	year = {2019}}

@article{ishida_flooding_loss,
	author = {Ishida, Takashi and Yamane, Ikko and Sakai, Tomoya and Niu, Gang and Sugiyama, Masashi},
	date-added = {2023-05-21 14:01:56 +0200},
	date-modified = {2023-05-21 14:03:51 +0200},
	journal = {arXiv preprint arXiv:2002.08709},
	keywords = {DoubleDescent},
	title = {Do we need zero training loss after achieving zero training error?},
	year = {2020}}

@article{nakkiran_DoubleDescent,
	author = {Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},
	date-added = {2023-05-21 14:01:56 +0200},
	date-modified = {2023-05-21 14:04:20 +0200},
	journal = {Journal of Statistical Mechanics: Theory and Experiment},
	keywords = {DoubleDescent},
	title = {Deep double descent: Where bigger models and more data hurt},
	year = {2021}}

@phdthesis{nakkiran_PHD,
	author = {Nakkiran, Preetum},
	date-added = {2023-05-21 14:01:56 +0200},
	date-modified = {2023-05-21 14:03:58 +0200},
	keywords = {DoubleDescent},
	school = {Harvard University},
	title = {Towards an Empirical Theory of Deep Learning},
	year = {2021}}

@article{Lee_VC_DoubleDescent,
	author = {Lee, Eng Hock and Cherkassky, Vladimir},
	date-added = {2023-05-21 14:01:56 +0200},
	date-modified = {2023-05-21 14:04:28 +0200},
	journal = {arXiv preprint arXiv:2205.15549},
	keywords = {DoubleDescent},
	title = {VC Theoretical Explanation of Double Descent},
	year = {2022}}

@article{loshchilov2016Cosine,
	abstract = {Propose le cosine learning rate avec warm restart},
	author = {Loshchilov, Ilya and Hutter, Frank},
	date-added = {2023-05-21 13:56:34 +0200},
	date-modified = {2023-09-02 02:22:05 +0200},
	journal = {arXiv preprint arXiv:1608.03983},
	keywords = {LLM Advance},
	title = {SGDR: Stochastic gradient descent with warm restarts},
	year = {2016}}

@inproceedings{smith2017Cyclical,
	abstract = {Propose les learning rates cycliques pour les r{\'e}seaux de neurones},
	author = {Smith, Leslie N},
	booktitle = {2017 IEEE winter conference on applications of computer vision (WACV)},
	date-added = {2023-05-21 13:55:14 +0200},
	date-modified = {2023-09-02 02:20:34 +0200},
	keywords = {LLM Advance},
	organization = {IEEE},
	pages = {464--472},
	title = {Cyclical learning rates for training neural networks},
	year = {2017}}

@inproceedings{Glorot2010Initialization,
	abstract = {D{\'e}crit l'initialisation Glorot pour des r{\'e}seaux de neurones avec des fonctions d'activation impaire},
	author = {Glorot, Xavier and Bengio, Yoshua},
	booktitle = {Proceedings of the thirteenth international conference on artificial intelligence and statistics},
	date-added = {2023-05-21 13:53:16 +0200},
	date-modified = {2023-09-02 02:17:30 +0200},
	keywords = {GoodStart},
	organization = {JMLR Workshop and Conference Proceedings},
	pages = {249--256},
	title = {Understanding the difficulty of training deep feedforward neural networks},
	year = {2010}}

@inproceedings{he2015Initialization,
	abstract = {Description de l'initialisation des poids He pour des r{\'e}seaux de neurones ReLU},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	booktitle = {Proceedings of the IEEE international conference on computer vision},
	date-added = {2023-05-21 13:52:14 +0200},
	date-modified = {2023-09-02 02:16:44 +0200},
	keywords = {GoodStart},
	pages = {1026--1034},
	title = {Delving deep into rectifiers: Surpassing human-level performance on imagenet classification},
	year = {2015}}

@article{achille2017Critical,
	abstract = {Montre que le d{\'e}but des r{\'e}seaux de neurones est critique et qu'un retard n'est pas rattrap{\'e}. Forte analogie avec la vision dans le monde vivant},
	author = {Achille, Alessandro and Rovere, Matteo and Soatto, Stefano},
	date-added = {2023-05-21 13:49:46 +0200},
	date-modified = {2023-09-02 02:16:10 +0200},
	journal = {arXiv preprint arXiv:1711.08856},
	keywords = {GoodStart},
	title = {Critical learning periods in deep neural networks},
	year = {2017}}
